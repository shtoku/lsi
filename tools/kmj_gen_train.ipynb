{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import kmj_gen_np as kgn\n",
    "from layers import *\n",
    "from xorshift_train import XorShift\n",
    "from collections import OrderedDict\n",
    "np.set_printoptions(precision=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DEC = '../data/parameter/train/decimal/'\n",
    "\n",
    "\n",
    "N = 10              # 最大文字数\n",
    "char_num = 72       # 文字種数\n",
    "emb_dim = 12        # 文字ベクトルの次元\n",
    "hid_dim = 12        # 潜在ベクトルの次元\n",
    "\n",
    "\n",
    "batch_size = 32     # ミニバッチサイズ\n",
    "n_epochs = 35       # エポック数\n",
    "lr = 1e-3           # 学習率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "  def __init__(self):\n",
    "    self.params = {}\n",
    "    self.params['W_emb'] = convert_fixed(kgn.read_param(PATH_DEC + 'emb_layer_W_emb.txt').reshape(char_num, emb_dim))\n",
    "\n",
    "    W_1 = convert_fixed(kgn.read_param(PATH_DEC + 'mix_layer_W_1.txt').reshape(emb_dim, N, hid_dim))\n",
    "    self.params['W_1'] = np.concatenate([W_1, np.zeros((hid_dim, hid_dim-N, hid_dim))], axis=1)\n",
    "    self.params['b_1'] = convert_fixed(kgn.read_param(PATH_DEC + 'mix_layer_b_1.txt').reshape(emb_dim, hid_dim))\n",
    "\n",
    "    W_2 = convert_fixed(kgn.read_param(PATH_DEC + 'mix_layer_W_2.txt').reshape(hid_dim, emb_dim, 1))\n",
    "    self.params['W_2'] = np.concatenate([W_2, np.zeros((hid_dim, hid_dim, hid_dim-1))], axis=2)\n",
    "    b_2 = convert_fixed(kgn.read_param(PATH_DEC + 'mix_layer_b_2.txt').reshape(hid_dim, 1))\n",
    "    self.params['b_2'] = np.concatenate([b_2, np.zeros((hid_dim, hid_dim-1))], axis=1)\n",
    "\n",
    "    W_3 = convert_fixed(kgn.read_param(PATH_DEC + 'mix_layer_W_3.txt').reshape(N, hid_dim, hid_dim))\n",
    "    self.params['W_3'] = np.concatenate([W_3, np.zeros((hid_dim-N, hid_dim, hid_dim))], axis=0)\n",
    "    b_3 = convert_fixed(kgn.read_param(PATH_DEC + 'mix_layer_b_3.txt').reshape(N, hid_dim))\n",
    "    self.params['b_3'] = np.concatenate([b_3, np.zeros((hid_dim-N, hid_dim))], axis=0)\n",
    "\n",
    "    self.params['W_out'] = convert_fixed(kgn.read_param(PATH_DEC + 'dense_layer_W_out.txt').reshape(hid_dim, char_num))\n",
    "\n",
    "    self.grads = {}\n",
    "    self.zero_grads()\n",
    "\n",
    "    self.layers = OrderedDict()\n",
    "    self.layers['Emb_Layer'] = Emb_Layer(self.params['W_emb'])\n",
    "    self.layers['Mix_Layer1'] = Mix_Layer(self.params['W_1'], self.params['b_1'], 1)\n",
    "    self.layers['Tanh_Layer1'] = Tanh_Layer()\n",
    "    self.layers['Mix_Layer2'] = Mix_Layer(self.params['W_2'], self.params['b_2'], 2)\n",
    "    self.layers['Tanh_Layer2'] = Tanh_Layer()\n",
    "    self.layers['Mix_Layer3'] = Mix_Layer(self.params['W_3'], self.params['b_3'], 3)\n",
    "    self.layers['Tanh_Layer3'] = Tanh_Layer()\n",
    "    self.layers['Dense_Layer'] = Dense_Layer(self.params['W_out'])\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.layers['Emb_Layer'].forward(x)\n",
    "    x = np.concatenate([x, np.zeros((hid_dim-N, hid_dim))], axis=0)\n",
    "    x = self.layers['Mix_Layer1'].forward(x)\n",
    "    x = self.layers['Tanh_Layer1'].forward(x)\n",
    "    x = self.layers['Mix_Layer2'].forward(x)\n",
    "    x = self.layers['Tanh_Layer2'].forward(x)\n",
    "    x = np.full((hid_dim, hid_dim), x[:, 0]).T\n",
    "    x = self.layers['Mix_Layer3'].forward(x)\n",
    "    x = self.layers['Tanh_Layer3'].forward(x)\n",
    "    x = x[:N, :]\n",
    "    x = self.layers['Dense_Layer'].forward(x)\n",
    "    \n",
    "    return x\n",
    "  \n",
    "  def zero_grads(self):\n",
    "    self.grads['W_emb'] = np.zeros_like(self.params['W_emb'])\n",
    "    self.grads['W_1'] = np.zeros_like(self.params['W_1'])\n",
    "    self.grads['b_1'] = np.zeros_like(self.params['b_1'])\n",
    "    self.grads['W_2'] = np.zeros_like(self.params['W_2'])\n",
    "    self.grads['b_2'] = np.zeros_like(self.params['b_2'])\n",
    "    self.grads['W_3'] = np.zeros_like(self.params['W_3'])\n",
    "    self.grads['b_3'] = np.zeros_like(self.params['b_3'])\n",
    "    self.grads['W_out'] = np.zeros_like(self.params['W_out'])\n",
    "\n",
    "  def gradient(self, y, t):\n",
    "    # Softmax\n",
    "    y = softmax(y)\n",
    "    y[range(len(y)), t] -= 1.0\n",
    "    dout = convert_fixed(y / batch_size)\n",
    "\n",
    "    dout = self.layers['Dense_Layer'].backward(dout)\n",
    "    dout = np.concatenate([dout, np.zeros((hid_dim-N, hid_dim))], axis=0)\n",
    "    dout = self.layers['Tanh_Layer3'].backward(dout)\n",
    "    dout = self.layers['Mix_Layer3'].backward(dout)\n",
    "    dout = dout.sum(axis=1, keepdims=True)\n",
    "    dout = np.concatenate([dout, np.zeros((hid_dim, hid_dim-1))], axis=1)\n",
    "    dout = self.layers['Tanh_Layer2'].backward(dout)\n",
    "    dout = self.layers['Mix_Layer2'].backward(dout)\n",
    "    dout = self.layers['Tanh_Layer1'].backward(dout)\n",
    "    dout = self.layers['Mix_Layer1'].backward(dout)\n",
    "    dout = dout[:N, :]\n",
    "    dout = self.layers['Emb_Layer'].backward(dout)\n",
    "    \n",
    "    self.grads['W_emb'] += self.layers['Emb_Layer'].dW\n",
    "    self.grads['W_1'] += self.layers['Mix_Layer1'].dW\n",
    "    self.grads['b_1'] += self.layers['Mix_Layer1'].db\n",
    "    self.grads['W_2'] += self.layers['Mix_Layer2'].dW\n",
    "    self.grads['b_2'] += self.layers['Mix_Layer2'].db\n",
    "    self.grads['W_3'] += self.layers['Mix_Layer3'].dW\n",
    "    self.grads['b_3'] += self.layers['Mix_Layer3'].db\n",
    "    self.grads['W_out'] += self.layers['Dense_Layer'].dW\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixed ver\n",
      "batch_size: 32, lr: 0.001, i_len:8, f_len: 16\n",
      "EPOCH:   1, Train Loss: 35.83966  Acc: 0.237, Valid Loss: 28.93384  Acc: 0.332\n",
      "EPOCH:   2, Train Loss: 27.77369  Acc: 0.343, Valid Loss: 26.86635  Acc: 0.361\n",
      "EPOCH:   3, Train Loss: 25.90408  Acc: 0.384, Valid Loss: 25.37519  Acc: 0.394\n",
      "EPOCH:   4, Train Loss: 24.79449  Acc: 0.411, Valid Loss: 24.48075  Acc: 0.416\n",
      "EPOCH:   5, Train Loss: 23.85992  Acc: 0.432, Valid Loss: 23.54161  Acc: 0.440\n",
      "EPOCH:   6, Train Loss: 22.92613  Acc: 0.453, Valid Loss: 22.66697  Acc: 0.456\n",
      "EPOCH:   7, Train Loss: 22.16878  Acc: 0.467, Valid Loss: 22.01457  Acc: 0.467\n",
      "EPOCH:   8, Train Loss: 21.60576  Acc: 0.476, Valid Loss: 21.52983  Acc: 0.473\n",
      "EPOCH:   9, Train Loss: 21.17331  Acc: 0.483, Valid Loss: 21.13464  Acc: 0.479\n",
      "EPOCH:  10, Train Loss: 20.81265  Acc: 0.489, Valid Loss: 20.78402  Acc: 0.487\n",
      "EPOCH:  11, Train Loss: 20.46899  Acc: 0.497, Valid Loss: 20.43951  Acc: 0.494\n",
      "EPOCH:  12, Train Loss: 20.11560  Acc: 0.506, Valid Loss: 20.07807  Acc: 0.502\n",
      "EPOCH:  13, Train Loss: 19.73894  Acc: 0.515, Valid Loss: 19.70425  Acc: 0.513\n",
      "EPOCH:  14, Train Loss: 19.35395  Acc: 0.526, Valid Loss: 19.34202  Acc: 0.521\n",
      "EPOCH:  15, Train Loss: 18.98958  Acc: 0.535, Valid Loss: 19.00413  Acc: 0.530\n",
      "EPOCH:  16, Train Loss: 18.65958  Acc: 0.543, Valid Loss: 18.70455  Acc: 0.538\n",
      "EPOCH:  17, Train Loss: 18.36001  Acc: 0.551, Valid Loss: 18.42200  Acc: 0.546\n",
      "EPOCH:  18, Train Loss: 18.08226  Acc: 0.558, Valid Loss: 18.15447  Acc: 0.554\n",
      "EPOCH:  19, Train Loss: 17.81355  Acc: 0.565, Valid Loss: 17.89731  Acc: 0.560\n",
      "EPOCH:  20, Train Loss: 17.55880  Acc: 0.573, Valid Loss: 17.65107  Acc: 0.566\n",
      "EPOCH:  21, Train Loss: 17.30876  Acc: 0.579, Valid Loss: 17.41484  Acc: 0.572\n",
      "EPOCH:  22, Train Loss: 17.06669  Acc: 0.585, Valid Loss: 17.18635  Acc: 0.579\n",
      "EPOCH:  23, Train Loss: 16.83463  Acc: 0.590, Valid Loss: 16.95788  Acc: 0.583\n",
      "EPOCH:  24, Train Loss: 16.60815  Acc: 0.594, Valid Loss: 16.74205  Acc: 0.587\n",
      "EPOCH:  25, Train Loss: 16.39556  Acc: 0.598, Valid Loss: 16.53649  Acc: 0.590\n",
      "EPOCH:  26, Train Loss: 16.19171  Acc: 0.601, Valid Loss: 16.33834  Acc: 0.594\n",
      "EPOCH:  27, Train Loss: 16.00071  Acc: 0.605, Valid Loss: 16.15114  Acc: 0.597\n",
      "EPOCH:  28, Train Loss: 15.82245  Acc: 0.609, Valid Loss: 15.97905  Acc: 0.601\n",
      "EPOCH:  29, Train Loss: 15.65240  Acc: 0.612, Valid Loss: 15.81894  Acc: 0.605\n",
      "EPOCH:  30, Train Loss: 15.49507  Acc: 0.615, Valid Loss: 15.66519  Acc: 0.609\n",
      "EPOCH:  31, Train Loss: 15.34323  Acc: 0.618, Valid Loss: 15.51692  Acc: 0.612\n",
      "EPOCH:  32, Train Loss: 15.19907  Acc: 0.621, Valid Loss: 15.37692  Acc: 0.614\n",
      "EPOCH:  33, Train Loss: 15.06325  Acc: 0.625, Valid Loss: 15.24350  Acc: 0.618\n",
      "EPOCH:  34, Train Loss: 14.93149  Acc: 0.628, Valid Loss: 15.11154  Acc: 0.620\n",
      "EPOCH:  35, Train Loss: 14.80625  Acc: 0.631, Valid Loss: 14.99198  Acc: 0.623\n"
     ]
    }
   ],
   "source": [
    "# データセットの読み込み\n",
    "kmj_dataset = kgn.read_dataset('../data/dataset/kaomoji_MAX=10_DA.txt')\n",
    "\n",
    "# データの前処理\n",
    "kmj_onehot = kgn.preprocess(kmj_dataset)\n",
    "kmj_int = kmj_onehot.argmax(axis=-1)\n",
    "\n",
    "# データセットを分割\n",
    "train_size = int(len(kmj_dataset) * 0.85)\n",
    "valid_size = int(len(kmj_dataset) * 0.10)\n",
    "test_size  = len(kmj_dataset) - train_size - valid_size\n",
    "\n",
    "dataset_train = kmj_int[:train_size]\n",
    "dataset_valid = kmj_int[train_size:train_size+valid_size]\n",
    "dataset_test  = kmj_int[train_size+valid_size:]\n",
    "\n",
    "# ミニバッチに分割\n",
    "n_train = int(train_size / batch_size)\n",
    "n_valid = int(valid_size / batch_size)\n",
    "\n",
    "dataloader_train = [dataset_train[i*batch_size:(i+1)*batch_size] for i in range(n_train)]\n",
    "dataloader_valid = [dataset_valid[i*batch_size:(i+1)*batch_size] for i in range(n_valid)]\n",
    "\n",
    "# dataloader_train.append(dataset_train[n_train*batch_size:])\n",
    "# dataloader_valid.append(dataset_valid[n_valid*batch_size:])\n",
    "\n",
    "# モデルの定義\n",
    "net = Network()\n",
    "optim = Momentum(lr=lr)\n",
    "\n",
    "# 学習\n",
    "print('fixed ver')\n",
    "print('batch_size: {:}, lr: {:}, i_len:{:}, f_len: {:}'.format(batch_size, lr, i_len, f_len))\n",
    "for epoch in range(n_epochs):\n",
    "  losses_train = []\n",
    "  losses_valid = []\n",
    "  acc_train = 0\n",
    "  acc_valid = 0\n",
    "\n",
    "  for batch in dataloader_train:\n",
    "    net.zero_grads()\n",
    "    for x in batch:\n",
    "      y = net.forward(x)\n",
    "      loss = crossEntropyLoss(y, x)\n",
    "      net.gradient(y, x)\n",
    "      losses_train.append(loss)\n",
    "      acc_train += (y.argmax(axis=-1) == x).sum()\n",
    "    \n",
    "    optim.update(net.params, net.grads)\n",
    "  \n",
    "  for batch in dataloader_valid:\n",
    "    net.zero_grads()\n",
    "    for x in batch:\n",
    "      y = net.forward(x)\n",
    "      loss = crossEntropyLoss(y, x)\n",
    "      losses_valid.append(loss)\n",
    "      acc_valid += (y.argmax(axis=-1) == x).sum()    \n",
    "    \n",
    "  if (epoch+1) % 1 == 0:\n",
    "    print('EPOCH: {:>3}, Train Loss: {:>8.5f}  Acc: {:>.3f}, Valid Loss: {:>8.5f}  Acc: {:>.3f}'.format(\n",
    "        epoch+1,\n",
    "        np.mean(losses_train),\n",
    "        acc_train / (train_size * N),\n",
    "        np.mean(losses_valid),\n",
    "        acc_valid / (valid_size * N)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base     : ヾ(*　∀́　*)ノ\n",
      "generate : !(*　∀́　*)ノ\n",
      "generate : ヾ(*　́　　*)ノ\n",
      "generate : !(*　ω　　*)\n",
      "generate : !(*　ω・　*)\n",
      "generate : ヾ(*　∀　　*)ノ\n",
      "generate : !(*　∀　　*)ノ\n",
      "generate : ヾ(*　^　　*)ノ\n",
      "generate : ヾ(*　ω　　*)ノ\n",
      "generate : ヾ(*　^　　))\n",
      "generate : ヾ(*　^́　))\n",
      "generate : !(*　́̄　))\n",
      "generate : ヾ(*　゚́　*))\n",
      "generate : !(*　́́　))\n",
      "generate : ヾ(*　ω　　))\n",
      "generate : ヾ(*　́́　*)ノ\n",
      "generate : ヾ(*　∀　　*)ノ\n",
      "generate : !(*　^　　*)ノ\n",
      "generate : ヾ(*　∀̄　*)ノ\n",
      "generate : ヾ(*　́　　*)ノ\n",
      "generate : ヾ(*　^́　))\n"
     ]
    }
   ],
   "source": [
    "# 類似生成テスト\n",
    "kmj = ['ヾ(*　∀́　*)ノ']\n",
    "print('base     :', ''.join(kmj))\n",
    "\n",
    "kmj = kgn.preprocess(kmj).argmax(axis=-1)\n",
    "xors = XorShift(6568)\n",
    "\n",
    "for _ in range(20):\n",
    "  x = kmj[0]\n",
    "\n",
    "  x = net.layers['Emb_Layer'].forward(x)\n",
    "  x = np.concatenate([x, np.zeros((hid_dim-N, hid_dim))], axis=0)\n",
    "  x = net.layers['Mix_Layer1'].forward(x)\n",
    "  x = net.layers['Tanh_Layer1'].forward(x)\n",
    "  x = net.layers['Mix_Layer2'].forward(x)\n",
    "  x = net.layers['Tanh_Layer2'].forward(x)\n",
    "\n",
    "  noise = np.empty((hid_dim,))\n",
    "  for i in range(hid_dim):\n",
    "    noise[i] = xors() / 4\n",
    "  noise = convert_fixed(noise)\n",
    "  z = x[:, 0] + noise\n",
    "\n",
    "  x = np.full((hid_dim, hid_dim), z).T\n",
    "  x = net.layers['Mix_Layer3'].forward(x)\n",
    "  x = net.layers['Tanh_Layer3'].forward(x)\n",
    "  x = x[:N, :]\n",
    "  y = net.layers['Dense_Layer'].forward(x)    \n",
    "\n",
    "  print('generate :', kgn.convert_str(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed : 9659 new_gen : !*^^゚)・))\n",
      "seed : 1488 new_gen : ((　　)　))\n",
      "seed : 2183 new_gen : (　\n",
      "seed : 1294 new_gen : !　(　^́^　)\n",
      "seed : 2620 new_gen : !)(*(・́　)\n",
      "seed : 2188 new_gen : (　́)\n",
      "seed : 2371 new_gen : )(^　)\n",
      "seed : 6869 new_gen : *(())(\n",
      "seed : 8279 new_gen : !*(`・　゚*)\n",
      "seed : 3099 new_gen : (^(・)・・\n",
      "seed : 3720 new_gen : ヾ(・(\n",
      "seed : 9726 new_gen : ・　(^(^　́　)\n",
      "seed : 1800 new_gen : *(・　\n",
      "seed : 9930 new_gen : ^^)^^\n",
      "seed : 8629 new_gen : (　・・)\n",
      "seed : 1285 new_gen : 　(*　))(\n",
      "seed : 6788 new_gen : (*・^・̄　)\n",
      "seed : 7668 new_gen : *)()　))\n",
      "seed : 7271 new_gen : ^(^(^　́́)\n",
      "seed : 8958 new_gen : )!!(　)^)\n",
      "seed : 3789 new_gen : !)*(^　゚*))\n",
      "seed : 6568 new_gen : !(^ω^・))\n",
      "seed : 7113 new_gen : !(^　^)・\n",
      "seed : 1140 new_gen : ^)()・\n",
      "seed : 4616 new_gen : (・^)\n"
     ]
    }
   ],
   "source": [
    "# 新規生成テスト\n",
    "import random\n",
    "\n",
    "for seed in random.sample(range(1000, 10000), 25):\n",
    "  xors = XorShift(seed)\n",
    "  z = np.empty((hid_dim,))\n",
    "  for i in range(hid_dim):\n",
    "    z[i] = xors()\n",
    "  z = convert_fixed(z)\n",
    "\n",
    "  z = np.full((hid_dim, hid_dim), z).T\n",
    "  y = net.layers['Mix_Layer3'].forward(z)\n",
    "  y = net.layers['Tanh_Layer3'].forward(y)\n",
    "  y = net.layers['Dense_Layer'].forward(y)\n",
    "\n",
    "  print('seed :', seed, 'new_gen :', kgn.convert_str(y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
